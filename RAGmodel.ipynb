{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "import tempfile\n",
    "import whisper\n",
    "import langid\n",
    "import shutil\n",
    "from moviepy.editor import VideoFileClip\n",
    "\n",
    "#from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Ollama(model='llama3.1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the output parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = StrOutputParser() # Output parser for the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the template for the translation/summarization task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_translation = ChatPromptTemplate.from_template(\"\"\"\n",
    "    Context: {context}\n",
    "    Translate the following {text} from {source_language} to {target_language}.\n",
    "\"\"\")\n",
    "\n",
    "prompt_summary = ChatPromptTemplate.from_template(\"\"\"\n",
    "    Summarize{answer}.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiating whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef extract_audio_from_video(video_path, output_audio_path):\\n    # Extracts the audio from a video file and saves it as an audio file.\\n    video_clip = VideoFileClip(video_path)\\n    video_clip.audio.write_audiofile(output_audio_path, codec=\\'pcm_s16le\\')\\n    video_clip.close()\\n\\ndef transcribe_audio(audio_path):\\n    # Transcribes the audio file using Whisper.\\n    whisper_model = whisper.load_model(\"small\")\\n    return whisper_model.transcribe(audio_path, fp16=False)[\"text\"].strip()\\n\\nfile_path = \"french.mp3\"  \\n\\nif file_path.lower().endswith((\\'.mp4\\', \\'.mkv\\', \\'.avi\\')):\\n    audio_file = \\'temp_audio.wav\\' \\n    extract_audio_from_video(file_path, audio_file)\\n    transcription = transcribe_audio(audio_file)\\n    os.remove(audio_file)  \\n#elif file_path.lower().endswith((\\'.mp3\\', \\'.wav\\', \\'.m4a\\')):\\n#    transcription = transcribe_audio(file_path)\\nelse:\\n    raise ValueError(f\"The file format of {file_path} is not supported.\")\\n\\ntranscription = transcribe_audio(file_path)\\n\\ntranscription_file = \"transcription.txt\"\\nwith open(transcription_file, \"w\") as file:\\n    file.write(transcription)\\n\\n\\nwith open(transcription_file, \"r\") as file:\\n    transcript = file.read()\\nprint(transcript)\\n\\n\\nlanguage = langid.classify(transcript)[0]\\nprint(\"Detected Language:\", language)\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def extract_audio_from_video(video_path, output_audio_path):\n",
    "    # Extracts the audio from a video file and saves it as an audio file.\n",
    "    video_clip = VideoFileClip(video_path)\n",
    "    video_clip.audio.write_audiofile(output_audio_path, codec='pcm_s16le')\n",
    "    video_clip.close()\n",
    "\n",
    "def transcribe_audio(audio_path):\n",
    "    # Transcribes the audio file using Whisper.\n",
    "    whisper_model = whisper.load_model(\"small\")\n",
    "    return whisper_model.transcribe(audio_path, fp16=False)[\"text\"].strip()\n",
    "\n",
    "file_path = \"french.mp3\"  \n",
    "\n",
    "if file_path.lower().endswith(('.mp4', '.mkv', '.avi')):\n",
    "    audio_file = 'temp_audio.wav' \n",
    "    extract_audio_from_video(file_path, audio_file)\n",
    "    transcription = transcribe_audio(audio_file)\n",
    "    os.remove(audio_file)  \n",
    "#elif file_path.lower().endswith(('.mp3', '.wav', '.m4a')):\n",
    "#    transcription = transcribe_audio(file_path)\n",
    "else:\n",
    "    raise ValueError(f\"The file format of {file_path} is not supported.\")\n",
    "\n",
    "transcription = transcribe_audio(file_path)\n",
    "\n",
    "transcription_file = \"transcription.txt\"\n",
    "with open(transcription_file, \"w\") as file:\n",
    "    file.write(transcription)\n",
    "\n",
    "\n",
    "with open(transcription_file, \"r\") as file:\n",
    "    transcript = file.read()\n",
    "print(transcript)\n",
    "\n",
    "\n",
    "language = langid.classify(transcript)[0]\n",
    "print(\"Detected Language:\", language)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breaking the transcript into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'transcription.txt'}, page_content='Numéro 1. Toucher un crayon. Numéro 2. Toucher une fenêtre. Numéro 3. Montrez-moi une table. Numéro 4. Toucher une porte. Numéro 5. Montrez-moi un cahier. Numéro 6. Montrez-moi un ordinateur. Numéro 7. Toucher une chaise. Numéro 8. Montrez-moi un stylo.')]\n"
     ]
    }
   ],
   "source": [
    "loader = TextLoader(\"transcription.txt\")\n",
    "text_documents = loader.load()\n",
    "text_documents\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(text_documents)\n",
    "\n",
    "print(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_embedder() :\n",
    "     model_name = 'intfloat/multilingual-e5-base'\n",
    "     return HuggingFaceEmbeddings(\n",
    "        model_name=model_name,\n",
    "    )\n",
    "\n",
    "hf = make_embedder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Text: Numéro 1. Toucher un crayon. Numéro 2. Toucher une fenêtre. Numéro 3. Montrez-moi une table. Numéro 4. Toucher une porte. Numéro 5. Montrez-moi un cahier. Numéro 6. Montrez-moi un ordinateur. Numéro 7. Toucher une chaise. Numéro 8. Montrez-moi un stylo.\n"
     ]
    }
   ],
   "source": [
    "document = texts[0]\n",
    "text = document.page_content  # Extracting text\n",
    "\n",
    "print(\"Extracted Text:\", text)\n",
    "\n",
    "vectorstore2 = DocArrayInMemorySearch.from_documents(texts, hf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Result: To translate the given text from French to English, I'll proceed step by step.\n",
      "\n",
      "Given text:\n",
      "```\n",
      "Numéro 1. Toucher un crayon.\n",
      "Numéro 2. Toucher une fenêtre.\n",
      "Numéro 3. Montrez-moi une table.\n",
      "Numéro 4. Toucher une porte.\n",
      "Numéro 5. Montrez-moi un cahier.\n",
      "Numéro 6. Montrez-moi un ordinateur.\n",
      "Numéro 7. Toucher une chaise.\n",
      "Numéro 8. Montrez-moi un stylo.\n",
      "```\n",
      "\n",
      "Translation:\n",
      "1. **Number 1. Touch a pencil.**\n",
      "2. **Number 2. Touch a window.**\n",
      "3. **Number 3. Show me a table.**\n",
      "4. **Number 4. Touch a door.**\n",
      "5. **Number 5. Show me a notebook.** ( Cahier can also mean \"notebook\" in French, or more generally it could refer to any kind of notebook but here given context suggests 'show me a notebook')\n",
      "6. **Number 6. Show me a computer.**\n",
      "7. **Number 7. Touch a chair.**\n",
      "8. **Number 8. Show me a pen.** (Stylo in French can also mean \"pen\" as opposed to crayon or pencil, so the translation might slightly vary depending on whether you're referring specifically to a writing instrument that uses ink like a pen or more generally to a drawing tool.)\n",
      "\n",
      "This translation should match the context of items in an office setting.\n",
      "\n",
      "Summary Result:  The human has taken the given text from French to English and broken it down step by step, understanding the context and translating each line individually.\n",
      "\n",
      "Here is a summary of the key points:\n",
      "\n",
      "1. The text appears to be a list of instructions related to an office setting.\n",
      "2. Each task is numbered and consists of touching/pointing to a specific object in the office.\n",
      "3. The translations were done line by line, taking into account the context of each item.\n",
      "4. In some cases (e.g., showing a table or notebook), it's more likely to show the object rather than touch it.\n",
      "\n",
      "The final output is an English translation of the original French text, which matches the context of items in an office setting.\n",
      "\n",
      "If you'd like me to summarize anything specific, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "source_language = \"fr\" #language\n",
    "target_language = input(\"Enter the target language: \")\n",
    "\n",
    "translation_chain = RunnablePassthrough() | prompt_translation | model | parser \n",
    "summary_chain = {\"answer\": translation_chain} | prompt_summary | model | parser \n",
    "\n",
    "try:\n",
    "    result = translation_chain.invoke({\n",
    "        \"context\": vectorstore2.as_retriever(),\n",
    "        \"text\": text,\n",
    "        \"source_language\": source_language,\n",
    "        \"target_language\": target_language\n",
    "    })\n",
    "    \n",
    "    print(\"Processed Result:\", result)\n",
    "\n",
    "    resultSum = summary_chain.invoke({\n",
    "        \"context\": vectorstore2.as_retriever(),\n",
    "        \"text\": result,\n",
    "        \"source_language\": source_language,\n",
    "        \"target_language\": target_language\n",
    "    })\n",
    "    print(\"\\nSummary Result: \", resultSum)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error invoking chain: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
